{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyusheeMittal/legendary-guide/blob/master/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "8983a915-2f70-455e-a3b0-eb83a8a55473",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "outputId": "6e4b7b31-56e6-479b-f51b-5800ef83a143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import argparse\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout, Flatten, Dense\n",
        "from keras.layers import Input, Conv2D, BatchNormalization, Activation, AveragePooling2D, MaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(8,2,figsize=(15,40))\n",
        "    # summarize history for accuracy\n",
        "    axs[0,0].plot(range(1,len(model_history.history['gender_output_acc'])+1),model_history.history['gender_output_acc'])\n",
        "    axs[0,0].plot(range(1,len(model_history.history['val_gender_output_acc'])+1),model_history.history['val_gender_output_acc'])\n",
        "    axs[0,0].set_title('Model gender_output_acc')\n",
        "    axs[0,0].set_ylabel('gender_output_acc')\n",
        "    axs[0,0].set_xlabel('Epoch')\n",
        "    axs[0,0].set_xticks(np.arange(1,len(model_history.history['gender_output_acc'])+1),len(model_history.history['gender_output_acc'])/10)\n",
        "    axs[0,0].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[0,1].plot(range(1,len(model_history.history['image_quality_output_acc'])+1),model_history.history['image_quality_output_acc'])\n",
        "    axs[0,1].plot(range(1,len(model_history.history['val_image_quality_output_acc'])+1),model_history.history['val_image_quality_output_acc'])\n",
        "    axs[0,1].set_title('Model image_quality_output_acc')\n",
        "    axs[0,1].set_ylabel('image_quality_output_acc')\n",
        "    axs[0,1].set_xlabel('Epoch')\n",
        "    axs[0,1].set_xticks(np.arange(1,len(model_history.history['image_quality_output_acc'])+1),len(model_history.history['image_quality_output_acc'])/10)\n",
        "    axs[0,1].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[1,0].plot(range(1,len(model_history.history['pose_output_loss'])+1),model_history.history['pose_output_loss'])\n",
        "    axs[1,0].plot(range(1,len(model_history.history['val_pose_output_loss'])+1),model_history.history['val_pose_output_loss'])\n",
        "    axs[1,0].set_title('Model Loss')\n",
        "    axs[1,0].set_ylabel('Loss')\n",
        "    axs[1,0].set_xlabel('Epoch')\n",
        "    axs[1,0].set_xticks(np.arange(1,len(model_history.history['gender_output_loss'])+1),len(model_history.history['gender_output_loss'])/10)\n",
        "    axs[1,0].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[1,1].plot(range(1,len(model_history.history['emotion_output_loss'])+1),model_history.history['emotion_output_loss'])   \n",
        "    axs[1,1].plot(range(1,len(model_history.history['val_emotion_output_loss'])+1),model_history.history['val_emotion_output_loss'])\n",
        "    axs[1,1].set_title('Model Loss')\n",
        "    axs[1,1].set_ylabel('Loss')\n",
        "    axs[1,1].set_xlabel('Epoch')\n",
        "    axs[1,1].set_xticks(np.arange(1,len(model_history.history['gender_output_loss'])+1),len(model_history.history['gender_output_loss'])/10)\n",
        "    axs[1,1].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[2,0].plot(range(1,len(model_history.history['age_output_acc'])+1),model_history.history['age_output_acc'])\n",
        "    axs[2,0].plot(range(1,len(model_history.history['val_age_output_acc'])+1),model_history.history['val_age_output_acc'])\n",
        "    axs[2,0].set_title('Model age_output_acc')\n",
        "    axs[2,0].set_ylabel('age_output_acc')\n",
        "    axs[2,0].set_xlabel('Epoch')\n",
        "    axs[2,0].set_xticks(np.arange(1,len(model_history.history['age_output_acc'])+1),len(model_history.history['age_output_acc'])/10)\n",
        "    axs[2,0].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[2,1].plot(range(1,len(model_history.history['weight_output_acc'])+1),model_history.history['weight_output_acc'])\n",
        "    axs[2,1].plot(range(1,len(model_history.history['val_weight_output_acc'])+1),model_history.history['val_weight_output_acc'])\n",
        "    axs[2,1].set_title('Model weight_output_acc')\n",
        "    axs[2,1].set_ylabel('weight_output_acc')\n",
        "    axs[2,1].set_xlabel('Epoch')\n",
        "    axs[2,1].set_xticks(np.arange(1,len(model_history.history['weight_output_acc'])+1),len(model_history.history['weight_output_acc'])/10)\n",
        "    axs[2,1].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[3,0].plot(range(1,len(model_history.history['bag_output_acc'])+1),model_history.history['bag_output_acc'])\n",
        "    axs[3,0].plot(range(1,len(model_history.history['val_bag_output_acc'])+1),model_history.history['val_bag_output_acc'])\n",
        "    axs[3,0].set_title('Model bag_output_acc')\n",
        "    axs[3,0].set_ylabel('bag_output_acc')\n",
        "    axs[3,0].set_xlabel('Epoch')\n",
        "    axs[3,0].set_xticks(np.arange(1,len(model_history.history['bag_output_acc'])+1),len(model_history.history['bag_output_acc'])/10)\n",
        "    axs[3,0].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[3,1].plot(range(1,len(model_history.history['footwear_output_acc'])+1),model_history.history['footwear_output_acc'])\n",
        "    axs[3,1].plot(range(1,len(model_history.history['val_footwear_output_acc'])+1),model_history.history['val_footwear_output_acc'])\n",
        "    axs[3,1].set_title('Model footwear_output_acc')\n",
        "    axs[3,1].set_ylabel('footwear_output_acc')\n",
        "    axs[3,1].set_xlabel('Epoch')\n",
        "    axs[3,1].set_xticks(np.arange(1,len(model_history.history['footwear_output_acc'])+1),len(model_history.history['footwear_output_acc'])/10)\n",
        "    axs[3,1].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[4,0].plot(range(1,len(model_history.history['pose_output_acc'])+1),model_history.history['pose_output_acc'])\n",
        "    axs[4,0].plot(range(1,len(model_history.history['val_pose_output_acc'])+1),model_history.history['val_pose_output_acc'])\n",
        "    axs[4,0].set_title('Model pose_output_acc')\n",
        "    axs[4,0].set_ylabel('pose_output_acc')\n",
        "    axs[4,0].set_xlabel('Epoch')\n",
        "    axs[4,0].set_xticks(np.arange(1,len(model_history.history['pose_output_acc'])+1),len(model_history.history['pose_output_acc'])/10)\n",
        "    axs[4,0].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[4,1].plot(range(1,len(model_history.history['emotion_output_acc'])+1),model_history.history['emotion_output_acc'])   \n",
        "    axs[4,1].plot(range(1,len(model_history.history['val_emotion_output_acc'])+1),model_history.history['val_emotion_output_acc'])\n",
        "    axs[4,1].set_title('Model emotion_output_acc')\n",
        "    axs[4,1].set_ylabel('emotion_output_acc')\n",
        "    axs[4,1].set_xlabel('Epoch')\n",
        "    axs[4,1].set_xticks(np.arange(1,len(model_history.history['emotion_output_acc'])+1),len(model_history.history['emotion_output_acc'])/10)\n",
        "    axs[4,1].legend(['train', 'val'], loc='best')\n",
        "\n",
        "\n",
        "    # summarize history for loss\n",
        "    axs[5,0].plot(range(1,len(model_history.history['gender_output_loss'])+1),model_history.history['gender_output_loss'])\n",
        "    axs[5,0].plot(range(1,len(model_history.history['val_gender_output_loss'])+1),model_history.history['val_gender_output_loss'])\n",
        "    axs[5,0].set_title('Model Loss')\n",
        "    axs[5,0].set_ylabel('Loss')\n",
        "    axs[5,0].set_xlabel('Epoch')\n",
        "    axs[5,0].set_xticks(np.arange(1,len(model_history.history['gender_output_loss'])+1),len(model_history.history['gender_output_loss'])/10)\n",
        "    axs[5,0].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[5,1].plot(range(1,len(model_history.history['image_quality_output_loss'])+1),model_history.history['image_quality_output_loss'])\n",
        "    axs[5,1].plot(range(1,len(model_history.history['val_image_quality_output_loss'])+1),model_history.history['val_image_quality_output_loss'])\n",
        "    axs[5,1].set_title('Model Loss')\n",
        "    axs[5,1].set_ylabel('Loss')\n",
        "    axs[5,1].set_xlabel('Epoch')\n",
        "    axs[5,1].set_xticks(np.arange(1,len(model_history.history['gender_output_loss'])+1),len(model_history.history['gender_output_loss'])/10)\n",
        "    axs[5,1].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[6,0].plot(range(1,len(model_history.history['age_output_loss'])+1),model_history.history['age_output_loss'])\n",
        "    axs[6,0].plot(range(1,len(model_history.history['val_age_output_loss'])+1),model_history.history['val_age_output_loss'])\n",
        "    axs[6,0].set_title('Model Loss')\n",
        "    axs[6,0].set_ylabel('Loss')\n",
        "    axs[6,0].set_xlabel('Epoch')\n",
        "    axs[6,0].set_xticks(np.arange(1,len(model_history.history['gender_output_loss'])+1),len(model_history.history['gender_output_loss'])/10)\n",
        "    axs[6,0].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[6,1].plot(range(1,len(model_history.history['weight_output_loss'])+1),model_history.history['weight_output_loss'])\n",
        "    axs[6,1].plot(range(1,len(model_history.history['val_weight_output_loss'])+1),model_history.history['val_weight_output_loss'])\n",
        "    axs[6,1].set_title('Model Loss')\n",
        "    axs[6,1].set_ylabel('Loss')\n",
        "    axs[6,1].set_xlabel('Epoch')\n",
        "    axs[6,1].set_xticks(np.arange(1,len(model_history.history['gender_output_loss'])+1),len(model_history.history['gender_output_loss'])/10)\n",
        "    axs[6,1].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[7,0].plot(range(1,len(model_history.history['bag_output_loss'])+1),model_history.history['bag_output_loss'])\n",
        "    axs[7,0].plot(range(1,len(model_history.history['val_bag_output_loss'])+1),model_history.history['val_bag_output_loss'])\n",
        "    axs[7,0].set_title('Model Loss')\n",
        "    axs[7,0].set_ylabel('Loss')\n",
        "    axs[7,0].set_xlabel('Epoch')\n",
        "    axs[7,0].set_xticks(np.arange(1,len(model_history.history['gender_output_loss'])+1),len(model_history.history['gender_output_loss'])/10)\n",
        "    axs[7,0].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    axs[7,1].plot(range(1,len(model_history.history['footwear_output_loss'])+1),model_history.history['footwear_output_loss'])\n",
        "    axs[7,1].plot(range(1,len(model_history.history['val_footwear_output_loss'])+1),model_history.history['val_footwear_output_loss'])\n",
        "    axs[7,1].set_title('Model Loss')\n",
        "    axs[7,1].set_ylabel('Loss')\n",
        "    axs[7,1].set_xlabel('Epoch')\n",
        "    axs[7,1].set_xticks(np.arange(1,len(model_history.history['gender_output_loss'])+1),len(model_history.history['gender_output_loss'])/10)\n",
        "    axs[7,1].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "71d64761-792b-41cc-b657-5d108c08a8d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "0a1b94ae-26db-4299-80a2-f88fe3d07a02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, aug_list=[], incl_orig=True):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        #self.augmentation = augmentation\n",
        "        self.aug_list = aug_list\n",
        "        self.incl_orig = incl_orig\n",
        "        self.orig_len = int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.incl_orig:\n",
        "            delta = 1\n",
        "        else:\n",
        "            delta = 0\n",
        "        return self.orig_len * (len(self.aug_list) + delta)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        if not self.incl_orig :\n",
        "            index += self.orig_len - 1\n",
        "\n",
        "        if index > self.orig_len - 1:\n",
        "            aug = self.aug_list[index // self.orig_len - 1]\n",
        "            index %= self.orig_len\n",
        "        else:\n",
        "            aug = None\n",
        "\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        images = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])        \n",
        "        images = images.astype('float32')/255\n",
        "        images -= np.mean(images, axis=0)\n",
        "\n",
        "        if aug is not None:\n",
        "            images = aug.flow(images, shuffle=False).next()\n",
        "\n",
        "        #image /= np.std(image, axis=0)\n",
        "\n",
        "        # images = np.array([io.imread(data_dir / p) for p in df.iloc[:, -1]])\n",
        "        # mean = []\n",
        "        # std = []\n",
        "        # for i in range(images.shape[-1]):\n",
        "        #     pixels = images[:, :, :, i].ravel()\n",
        "        #     mean.append(np.mean(pixels))\n",
        "        #     std.append(np.std(pixels))\n",
        "        \n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        #print(image.shape)\n",
        "        #print(target)\n",
        "        return images, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "c56e1a22-f412-4a90-d6ec-67cbd75e9b22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15, random_state=1)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "4420bcc9-3b58-4d25-b259-97568879afd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>resized/59.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2106</th>\n",
              "      <td>resized/2107.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5206</th>\n",
              "      <td>resized/5207.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1163</th>\n",
              "      <td>resized/1164.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13534</th>\n",
              "      <td>resized/13536.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "58        resized/59.jpg              0  ...                        1              0\n",
              "2106    resized/2107.jpg              1  ...                        1              0\n",
              "5206    resized/5207.jpg              0  ...                        1              0\n",
              "1163    resized/1164.jpg              1  ...                        1              0\n",
              "13534  resized/13536.jpg              1  ...                        0              1\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32)#, augmentation=None)#ImageDataGenerator(horizontal_flip=True))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "outputId": "8e1e7a6b-bf52-4b20-9cdc-cc744dc94b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIDz53enOs0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_layer(inputs, num_filters=16, kernel_size=3, strides=1, activation='relu',\n",
        "                 batch_normalization=True, conv_first=True):\n",
        "    conv = Conv2D(num_filters, kernel_size=kernel_size, strides=strides,\n",
        "                  padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "outputId": "af524e9f-f61c-4b93-f796-6d2f58a37395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input_tensor=Input(shape=(224, 224, 3))\n",
        "\n",
        "#num_filters = 16\n",
        "#x = resnet_layer(inputs=input_tensor, num_filters=16, kernel_size=7, strides=2)\n",
        "#x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "#x = resnet_layer(inputs=x)\n",
        "## Instantiate the stack of residual units\n",
        "#for stack in range(3):\n",
        "#    for res_block in range(5):\n",
        "#        strides = 1\n",
        "#        if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "#            strides = 2  # downsample\n",
        "#        y = resnet_layer(inputs=x, num_filters=num_filters, strides=strides)\n",
        "#        y = resnet_layer(inputs=y, num_filters=num_filters, activation=None)\n",
        "#        if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "#            # linear projection residual shortcut connection to match\n",
        "#            # changed dims\n",
        "#            x = resnet_layer(inputs=x, num_filters=num_filters, kernel_size=1,\n",
        "#                             strides=strides, activation=None, batch_normalization=False)\n",
        "#        x = keras.layers.add([x, y])\n",
        "#        x = Activation('relu')(x)\n",
        "#    num_filters *= 2\n",
        "\n",
        "x = Conv2D(16, 3, padding='same', kernel_regularizer=l2(1e-4), use_bias=False)(input_tensor)#3,224\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(32, 3, padding ='same', use_bias=False)(x)#5,224\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(64, 3, padding ='same', use_bias=False)(x)#7,224\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = MaxPooling2D(pool_size=(2,2))(x)#8,110\n",
        "\n",
        "x = Conv2D(16, 3, use_bias=False)(x)#12,108\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(32, 3, use_bias=False)(x)#16,106\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(64, 3, use_bias=False)(x)#20,104\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = MaxPooling2D(pool_size=(2,2))(x)#22,52\n",
        "\n",
        "x = Conv2D(16, 3, padding='same', use_bias=False)(x)#30,52\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(32, 3, padding ='same', use_bias=False)(x)#38,52\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(64, 3, padding ='same', use_bias=False)(x)#46,52\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = MaxPooling2D(pool_size=(2,2))(x)#50,26\n",
        "\n",
        "x = Conv2D(16, 3, use_bias=False)(x)#66,24\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(32, 3, use_bias=False)(x)#72,22\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(64, 3, use_bias=False)(x)#88,20\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = MaxPooling2D(pool_size=(2,2))(x)#96,10\n",
        "\n",
        "x = Conv2D(16, 3, padding='same', use_bias=False)(x)#128,10\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(32, 3, padding ='same', use_bias=False)(x)#160,10\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(64, 3, padding ='same', use_bias=False)(x)#192,10\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(64, 3, padding ='same', use_bias=False)(x)#224,10\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "x = Conv2D(8, 1, use_bias=False)(x)#224,10\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dropout(0.05)(x)\n",
        "\n",
        "\n",
        "x = AveragePooling2D(pool_size=2)(x)\n",
        "n = Flatten()(x)\n",
        "\n",
        "\n",
        "#neck = backbone.output\n",
        "#neck = Flatten(name=\"flatten\")(neck)\n",
        "#neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "#def build_tower(in_layer):\n",
        "#    neck = Dropout(0.2)(in_layer)\n",
        "#    neck = Dense(128, activation=\"relu\")(neck)\n",
        "#    neck = Dropout(0.3)(in_layer)\n",
        "#    neck = Dense(128, activation=\"relu\")(neck)\n",
        "#    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", n)\n",
        "image_quality = build_head(\"image_quality\", n)\n",
        "age = build_head(\"age\", n)\n",
        "weight = build_head(\"weight\", n)\n",
        "bag = build_head(\"bag\", n)\n",
        "footwear = build_head(\"footwear\", n)\n",
        "emotion = build_head(\"emotion\", n)\n",
        "pose = build_head(\"pose\", n)\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=input_tensor, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 224, 224, 16) 432         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 224, 224, 16) 64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 224, 224, 16) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 224, 224, 16) 0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 224, 224, 32) 4608        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 224, 224, 32) 128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 224, 224, 32) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 224, 224, 32) 0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 224, 224, 64) 18432       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 224, 224, 64) 256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 224, 224, 64) 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 224, 224, 64) 0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 112, 112, 64) 0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 110, 110, 16) 9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 110, 110, 16) 64          conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 110, 110, 16) 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 110, 110, 16) 0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 108, 108, 32) 4608        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 108, 108, 32) 128         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 108, 108, 32) 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 108, 108, 32) 0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 106, 106, 64) 18432       dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 106, 106, 64) 256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 106, 106, 64) 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 106, 106, 64) 0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 53, 53, 64)   0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 53, 53, 16)   9216        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 53, 53, 16)   64          conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 53, 53, 16)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 53, 53, 16)   0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 53, 53, 32)   4608        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 53, 53, 32)   128         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 53, 53, 32)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 53, 53, 32)   0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 53, 53, 64)   18432       dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 53, 53, 64)   256         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 53, 53, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 53, 53, 64)   0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 26, 26, 64)   0           dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 24, 24, 16)   9216        max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 24, 24, 16)   64          conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 24, 24, 16)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 24, 24, 16)   0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 22, 22, 32)   4608        dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 22, 22, 32)   128         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 22, 22, 32)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 22, 22, 32)   0           activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 20, 20, 64)   18432       dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 20, 20, 64)   256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 20, 20, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 20, 20, 64)   0           activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 10, 10, 64)   0           dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 10, 10, 16)   9216        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 10, 10, 16)   64          conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 10, 10, 16)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 10, 10, 16)   0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 10, 10, 32)   4608        dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 10, 10, 32)   128         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 10, 10, 32)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 10, 10, 32)   0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 10, 10, 64)   18432       dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 10, 10, 64)   256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 10, 10, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 10, 10, 64)   0           activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 10, 10, 64)   36864       dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 10, 10, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 10, 10, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 10, 10, 64)   0           activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 10, 10, 8)    512         dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 10, 10, 8)    32          conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 10, 10, 8)    0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 10, 10, 8)    0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 5, 5, 8)      0           dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 200)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "gender_output (Dense)           (None, 2)            402         flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "image_quality_output (Dense)    (None, 3)            603         flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "age_output (Dense)              (None, 5)            1005        flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "weight_output (Dense)           (None, 4)            804         flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bag_output (Dense)              (None, 3)            603         flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "footwear_output (Dense)         (None, 3)            603         flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "pose_output (Dense)             (None, 3)            603         flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "emotion_output (Dense)          (None, 4)            804         flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 197,827\n",
            "Trainable params: 196,563\n",
            "Non-trainable params: 1,264\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "outputId": "80312a09-65c6-4c06-9d33-d4eb23423ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "path = os.path.join(save_dir, 'model_{epoch:03d}.hdf5')\n",
        "print(path)\n",
        "\n",
        "checkpoint = ModelCheckpoint(path,\n",
        "                             save_best_only=True, verbose=1,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t monitor='val_loss', mode='min')\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t \n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, \n",
        "                               patience=3, min_lr=1e-6, verbose=1)\n",
        "#lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer]\n",
        "\n",
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "\n",
        "opt = SGD(lr=0.01, momentum=0.85, nesterov=True)\n",
        "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"] \n",
        "              # loss_weights=loss_weights\n",
        "             )"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/saved_models/model_{epoch:03d}.hdf5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "b75973d8-ee4c-494c-9072-0ca633e2ff75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_info= model.fit_generator(generator=train_gen, validation_data=valid_gen,\n",
        "    use_multiprocessing=True, workers=6, \n",
        "    epochs=20, \n",
        "    callbacks=callbacks, \n",
        "    verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.8414 - gender_output_loss: 0.6631 - image_quality_output_loss: 0.9907 - age_output_loss: 1.4318 - weight_output_loss: 0.9939 - bag_output_loss: 0.9208 - footwear_output_loss: 0.9981 - pose_output_loss: 0.9331 - emotion_output_loss: 0.9085 - gender_output_acc: 0.5995 - image_quality_output_acc: 0.5496 - age_output_acc: 0.3992 - weight_output_acc: 0.6325 - bag_output_acc: 0.5626 - footwear_output_acc: 0.5122 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7158Epoch 1/20\n",
            "360/360 [==============================] - 245s 681ms/step - loss: 7.8410 - gender_output_loss: 0.6631 - image_quality_output_loss: 0.9907 - age_output_loss: 1.4319 - weight_output_loss: 0.9944 - bag_output_loss: 0.9206 - footwear_output_loss: 0.9977 - pose_output_loss: 0.9327 - emotion_output_loss: 0.9086 - gender_output_acc: 0.5995 - image_quality_output_acc: 0.5497 - age_output_acc: 0.3990 - weight_output_acc: 0.6324 - bag_output_acc: 0.5624 - footwear_output_acc: 0.5128 - pose_output_acc: 0.6172 - emotion_output_acc: 0.7157 - val_loss: 7.8955 - val_gender_output_loss: 0.6473 - val_image_quality_output_loss: 0.9814 - val_age_output_loss: 1.4549 - val_weight_output_loss: 0.9833 - val_bag_output_loss: 0.9395 - val_footwear_output_loss: 0.9948 - val_pose_output_loss: 0.9290 - val_emotion_output_loss: 0.9640 - val_gender_output_acc: 0.6154 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5378 - val_footwear_output_acc: 0.5106 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 7.89555, saving model to /content/saved_models/model_001.hdf5\n",
            "Epoch 2/20\n",
            "360/360 [==============================] - 227s 630ms/step - loss: 7.6713 - gender_output_loss: 0.6359 - image_quality_output_loss: 0.9824 - age_output_loss: 1.4113 - weight_output_loss: 0.9789 - bag_output_loss: 0.9063 - footwear_output_loss: 0.9420 - pose_output_loss: 0.9142 - emotion_output_loss: 0.8989 - gender_output_acc: 0.6289 - image_quality_output_acc: 0.5522 - age_output_acc: 0.4001 - weight_output_acc: 0.6349 - bag_output_acc: 0.5627 - footwear_output_acc: 0.5620 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7161 - val_loss: 7.7956 - val_gender_output_loss: 0.6385 - val_image_quality_output_loss: 0.9819 - val_age_output_loss: 1.4294 - val_weight_output_loss: 0.9792 - val_bag_output_loss: 0.9323 - val_footwear_output_loss: 0.9520 - val_pose_output_loss: 0.9129 - val_emotion_output_loss: 0.9682 - val_gender_output_acc: 0.6310 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5393 - val_footwear_output_acc: 0.5539 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00002: val_loss improved from 7.89555 to 7.79558, saving model to /content/saved_models/model_002.hdf5\n",
            "Epoch 3/20\n",
            "360/360 [==============================] - 228s 632ms/step - loss: 7.5725 - gender_output_loss: 0.6173 - image_quality_output_loss: 0.9794 - age_output_loss: 1.4019 - weight_output_loss: 0.9736 - bag_output_loss: 0.8947 - footwear_output_loss: 0.9107 - pose_output_loss: 0.8979 - emotion_output_loss: 0.8955 - gender_output_acc: 0.6499 - image_quality_output_acc: 0.5514 - age_output_acc: 0.4039 - weight_output_acc: 0.6345 - bag_output_acc: 0.5721 - footwear_output_acc: 0.5836 - pose_output_acc: 0.6174 - emotion_output_acc: 0.7162 - val_loss: 7.6360 - val_gender_output_loss: 0.5972 - val_image_quality_output_loss: 0.9804 - val_age_output_loss: 1.4226 - val_weight_output_loss: 0.9661 - val_bag_output_loss: 0.9102 - val_footwear_output_loss: 0.9137 - val_pose_output_loss: 0.8797 - val_emotion_output_loss: 0.9648 - val_gender_output_acc: 0.6673 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3775 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5544 - val_footwear_output_acc: 0.5726 - val_pose_output_acc: 0.6134 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00003: val_loss improved from 7.79558 to 7.63605, saving model to /content/saved_models/model_003.hdf5\n",
            "Epoch 4/20\n",
            "360/360 [==============================] - 227s 630ms/step - loss: 7.4725 - gender_output_loss: 0.5910 - image_quality_output_loss: 0.9770 - age_output_loss: 1.3975 - weight_output_loss: 0.9689 - bag_output_loss: 0.8867 - footwear_output_loss: 0.8959 - pose_output_loss: 0.8614 - emotion_output_loss: 0.8927 - gender_output_acc: 0.6791 - image_quality_output_acc: 0.5535 - age_output_acc: 0.4068 - weight_output_acc: 0.6345 - bag_output_acc: 0.5754 - footwear_output_acc: 0.5865 - pose_output_acc: 0.6227 - emotion_output_acc: 0.7164 - val_loss: 7.6156 - val_gender_output_loss: 0.5914 - val_image_quality_output_loss: 0.9775 - val_age_output_loss: 1.4283 - val_weight_output_loss: 0.9796 - val_bag_output_loss: 0.9093 - val_footwear_output_loss: 0.9036 - val_pose_output_loss: 0.8493 - val_emotion_output_loss: 0.9753 - val_gender_output_acc: 0.6794 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3750 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5600 - val_footwear_output_acc: 0.5746 - val_pose_output_acc: 0.6205 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00004: val_loss improved from 7.63605 to 7.61560, saving model to /content/saved_models/model_004.hdf5\n",
            "Epoch 5/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.3807 - gender_output_loss: 0.5809 - image_quality_output_loss: 0.9751 - age_output_loss: 1.3940 - weight_output_loss: 0.9651 - bag_output_loss: 0.8782 - footwear_output_loss: 0.8850 - pose_output_loss: 0.8138 - emotion_output_loss: 0.8872 - gender_output_acc: 0.6915 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4014 - weight_output_acc: 0.6347 - bag_output_acc: 0.5857 - footwear_output_acc: 0.5960 - pose_output_acc: 0.6415 - emotion_output_acc: 0.7160\n",
            "360/360 [==============================] - 227s 631ms/step - loss: 7.3798 - gender_output_loss: 0.5806 - image_quality_output_loss: 0.9751 - age_output_loss: 1.3940 - weight_output_loss: 0.9652 - bag_output_loss: 0.8780 - footwear_output_loss: 0.8854 - pose_output_loss: 0.8136 - emotion_output_loss: 0.8868 - gender_output_acc: 0.6918 - image_quality_output_acc: 0.5538 - age_output_acc: 0.4016 - weight_output_acc: 0.6347 - bag_output_acc: 0.5859 - footwear_output_acc: 0.5957 - pose_output_acc: 0.6417 - emotion_output_acc: 0.7161 - val_loss: 7.6100 - val_gender_output_loss: 0.6364 - val_image_quality_output_loss: 0.9770 - val_age_output_loss: 1.4322 - val_weight_output_loss: 0.9699 - val_bag_output_loss: 0.9108 - val_footwear_output_loss: 0.9115 - val_pose_output_loss: 0.8144 - val_emotion_output_loss: 0.9566 - val_gender_output_acc: 0.6401 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3720 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.5585 - val_footwear_output_acc: 0.5746 - val_pose_output_acc: 0.6482 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00005: val_loss improved from 7.61560 to 7.61000, saving model to /content/saved_models/model_005.hdf5\n",
            "Epoch 6/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.2711 - gender_output_loss: 0.5571 - image_quality_output_loss: 0.9742 - age_output_loss: 1.3870 - weight_output_loss: 0.9633 - bag_output_loss: 0.8701 - footwear_output_loss: 0.8751 - pose_output_loss: 0.7609 - emotion_output_loss: 0.8821 - gender_output_acc: 0.7134 - image_quality_output_acc: 0.5528 - age_output_acc: 0.4053 - weight_output_acc: 0.6348 - bag_output_acc: 0.5982 - footwear_output_acc: 0.5958 - pose_output_acc: 0.6681 - emotion_output_acc: 0.7158\n",
            "Epoch 00005: val_loss improved from 7.61560 to 7.61000, saving model to /content/saved_models/model_005.hdf5\n",
            "360/360 [==============================] - 227s 631ms/step - loss: 7.2716 - gender_output_loss: 0.5574 - image_quality_output_loss: 0.9742 - age_output_loss: 1.3875 - weight_output_loss: 0.9630 - bag_output_loss: 0.8699 - footwear_output_loss: 0.8751 - pose_output_loss: 0.7612 - emotion_output_loss: 0.8820 - gender_output_acc: 0.7132 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4054 - weight_output_acc: 0.6348 - bag_output_acc: 0.5985 - footwear_output_acc: 0.5958 - pose_output_acc: 0.6680 - emotion_output_acc: 0.7159 - val_loss: 7.3758 - val_gender_output_loss: 0.5399 - val_image_quality_output_loss: 0.9790 - val_age_output_loss: 1.4119 - val_weight_output_loss: 0.9697 - val_bag_output_loss: 0.8932 - val_footwear_output_loss: 0.8898 - val_pose_output_loss: 0.7421 - val_emotion_output_loss: 0.9489 - val_gender_output_acc: 0.7193 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5761 - val_footwear_output_acc: 0.5847 - val_pose_output_acc: 0.6820 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00006: val_loss improved from 7.61000 to 7.37577, saving model to /content/saved_models/model_006.hdf5\n",
            "Epoch 7/20\n",
            "360/360 [==============================] - 228s 633ms/step - loss: 7.1741 - gender_output_loss: 0.5327 - image_quality_output_loss: 0.9745 - age_output_loss: 1.3815 - weight_output_loss: 0.9600 - bag_output_loss: 0.8619 - footwear_output_loss: 0.8618 - pose_output_loss: 0.7238 - emotion_output_loss: 0.8764 - gender_output_acc: 0.7298 - image_quality_output_acc: 0.5523 - age_output_acc: 0.4055 - weight_output_acc: 0.6341 - bag_output_acc: 0.6014 - footwear_output_acc: 0.6078 - pose_output_acc: 0.6875 - emotion_output_acc: 0.7162 - val_loss: 7.3435 - val_gender_output_loss: 0.5349 - val_image_quality_output_loss: 0.9768 - val_age_output_loss: 1.4230 - val_weight_output_loss: 0.9664 - val_bag_output_loss: 0.9012 - val_footwear_output_loss: 0.8869 - val_pose_output_loss: 0.7126 - val_emotion_output_loss: 0.9403 - val_gender_output_acc: 0.7303 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.3669 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5716 - val_footwear_output_acc: 0.5872 - val_pose_output_acc: 0.6961 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00007: val_loss improved from 7.37577 to 7.34349, saving model to /content/saved_models/model_007.hdf5\n",
            "Epoch 8/20\n",
            "360/360 [==============================] - 227s 632ms/step - loss: 7.0954 - gender_output_loss: 0.5131 - image_quality_output_loss: 0.9708 - age_output_loss: 1.3793 - weight_output_loss: 0.9566 - bag_output_loss: 0.8568 - footwear_output_loss: 0.8494 - pose_output_loss: 0.6954 - emotion_output_loss: 0.8726 - gender_output_acc: 0.7435 - image_quality_output_acc: 0.5550 - age_output_acc: 0.4076 - weight_output_acc: 0.6338 - bag_output_acc: 0.6049 - footwear_output_acc: 0.6187 - pose_output_acc: 0.7004 - emotion_output_acc: 0.7161 - val_loss: 7.3337 - val_gender_output_loss: 0.5171 - val_image_quality_output_loss: 0.9801 - val_age_output_loss: 1.4220 - val_weight_output_loss: 0.9659 - val_bag_output_loss: 0.8853 - val_footwear_output_loss: 0.8693 - val_pose_output_loss: 0.7345 - val_emotion_output_loss: 0.9582 - val_gender_output_acc: 0.7298 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.3816 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5867 - val_footwear_output_acc: 0.5973 - val_pose_output_acc: 0.6794 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00008: val_loss improved from 7.34349 to 7.33370, saving model to /content/saved_models/model_008.hdf5\n",
            "Epoch 9/20\n",
            "360/360 [==============================] - 227s 631ms/step - loss: 7.0398 - gender_output_loss: 0.5027 - image_quality_output_loss: 0.9702 - age_output_loss: 1.3764 - weight_output_loss: 0.9546 - bag_output_loss: 0.8500 - footwear_output_loss: 0.8441 - pose_output_loss: 0.6721 - emotion_output_loss: 0.8684 - gender_output_acc: 0.7532 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4064 - weight_output_acc: 0.6349 - bag_output_acc: 0.6142 - footwear_output_acc: 0.6155 - pose_output_acc: 0.7139 - emotion_output_acc: 0.7159 - val_loss: 7.2927 - val_gender_output_loss: 0.5203 - val_image_quality_output_loss: 0.9722 - val_age_output_loss: 1.4120 - val_weight_output_loss: 0.9632 - val_bag_output_loss: 0.8822 - val_footwear_output_loss: 0.8780 - val_pose_output_loss: 0.7217 - val_emotion_output_loss: 0.9416 - val_gender_output_acc: 0.7419 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3750 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5887 - val_footwear_output_acc: 0.5872 - val_pose_output_acc: 0.6900 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00009: val_loss improved from 7.33370 to 7.29272, saving model to /content/saved_models/model_009.hdf5\n",
            "Epoch 10/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.9669 - gender_output_loss: 0.4783 - image_quality_output_loss: 0.9670 - age_output_loss: 1.3704 - weight_output_loss: 0.9513 - bag_output_loss: 0.8467 - footwear_output_loss: 0.8339 - pose_output_loss: 0.6504 - emotion_output_loss: 0.8674 - gender_output_acc: 0.7690 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4078 - weight_output_acc: 0.6342 - bag_output_acc: 0.6126 - footwear_output_acc: 0.6241 - pose_output_acc: 0.7255 - emotion_output_acc: 0.7161\n",
            "Epoch 00009: val_loss improved from 7.33370 to 7.29272, saving model to /content/saved_models/model_009.hdf5\n",
            "360/360 [==============================] - 227s 632ms/step - loss: 6.9681 - gender_output_loss: 0.4785 - image_quality_output_loss: 0.9675 - age_output_loss: 1.3704 - weight_output_loss: 0.9513 - bag_output_loss: 0.8467 - footwear_output_loss: 0.8341 - pose_output_loss: 0.6507 - emotion_output_loss: 0.8674 - gender_output_acc: 0.7690 - image_quality_output_acc: 0.5530 - age_output_acc: 0.4077 - weight_output_acc: 0.6342 - bag_output_acc: 0.6126 - footwear_output_acc: 0.6240 - pose_output_acc: 0.7253 - emotion_output_acc: 0.7160 - val_loss: 7.2482 - val_gender_output_loss: 0.5299 - val_image_quality_output_loss: 0.9738 - val_age_output_loss: 1.4016 - val_weight_output_loss: 0.9616 - val_bag_output_loss: 0.8956 - val_footwear_output_loss: 0.8658 - val_pose_output_loss: 0.6762 - val_emotion_output_loss: 0.9423 - val_gender_output_acc: 0.7213 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3836 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5776 - val_footwear_output_acc: 0.5983 - val_pose_output_acc: 0.7147 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00010: val_loss improved from 7.29272 to 7.24824, saving model to /content/saved_models/model_010.hdf5\n",
            "Epoch 11/20\n",
            "360/360 [==============================] - 229s 635ms/step - loss: 6.9158 - gender_output_loss: 0.4651 - image_quality_output_loss: 0.9649 - age_output_loss: 1.3693 - weight_output_loss: 0.9506 - bag_output_loss: 0.8457 - footwear_output_loss: 0.8269 - pose_output_loss: 0.6287 - emotion_output_loss: 0.8632 - gender_output_acc: 0.7776 - image_quality_output_acc: 0.5556 - age_output_acc: 0.4082 - weight_output_acc: 0.6360 - bag_output_acc: 0.6139 - footwear_output_acc: 0.6276 - pose_output_acc: 0.7365 - emotion_output_acc: 0.7165 - val_loss: 7.4229 - val_gender_output_loss: 0.5834 - val_image_quality_output_loss: 0.9789 - val_age_output_loss: 1.4222 - val_weight_output_loss: 0.9679 - val_bag_output_loss: 0.9108 - val_footwear_output_loss: 0.8844 - val_pose_output_loss: 0.7310 - val_emotion_output_loss: 0.9427 - val_gender_output_acc: 0.6815 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.3740 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5706 - val_footwear_output_acc: 0.5907 - val_pose_output_acc: 0.6789 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 7.24824\n",
            "Epoch 12/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.8613 - gender_output_loss: 0.4543 - image_quality_output_loss: 0.9638 - age_output_loss: 1.3660 - weight_output_loss: 0.9470 - bag_output_loss: 0.8339 - footwear_output_loss: 0.8189 - pose_output_loss: 0.6145 - emotion_output_loss: 0.8613 - gender_output_acc: 0.7845 - image_quality_output_acc: 0.5539 - age_output_acc: 0.4102 - weight_output_acc: 0.6354 - bag_output_acc: 0.6261 - footwear_output_acc: 0.6329 - pose_output_acc: 0.7443 - emotion_output_acc: 0.7164\n",
            "360/360 [==============================] - 228s 635ms/step - loss: 6.8615 - gender_output_loss: 0.4547 - image_quality_output_loss: 0.9633 - age_output_loss: 1.3658 - weight_output_loss: 0.9470 - bag_output_loss: 0.8341 - footwear_output_loss: 0.8192 - pose_output_loss: 0.6140 - emotion_output_loss: 0.8619 - gender_output_acc: 0.7842 - image_quality_output_acc: 0.5543 - age_output_acc: 0.4105 - weight_output_acc: 0.6356 - bag_output_acc: 0.6261 - footwear_output_acc: 0.6327 - pose_output_acc: 0.7444 - emotion_output_acc: 0.7161 - val_loss: 7.1481 - val_gender_output_loss: 0.4589 - val_image_quality_output_loss: 0.9779 - val_age_output_loss: 1.4084 - val_weight_output_loss: 0.9590 - val_bag_output_loss: 0.8819 - val_footwear_output_loss: 0.8596 - val_pose_output_loss: 0.6580 - val_emotion_output_loss: 0.9427 - val_gender_output_acc: 0.7823 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3861 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.5852 - val_footwear_output_acc: 0.6094 - val_pose_output_acc: 0.7268 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00012: val_loss improved from 7.24824 to 7.14806, saving model to /content/saved_models/model_012.hdf5\n",
            "Epoch 13/20\n",
            "360/360 [==============================] - 227s 631ms/step - loss: 6.8075 - gender_output_loss: 0.4371 - image_quality_output_loss: 0.9589 - age_output_loss: 1.3645 - weight_output_loss: 0.9440 - bag_output_loss: 0.8299 - footwear_output_loss: 0.8126 - pose_output_loss: 0.6010 - emotion_output_loss: 0.8578 - gender_output_acc: 0.7957 - image_quality_output_acc: 0.5555 - age_output_acc: 0.4111 - weight_output_acc: 0.6356 - bag_output_acc: 0.6319 - footwear_output_acc: 0.6320 - pose_output_acc: 0.7497 - emotion_output_acc: 0.7160 - val_loss: 7.2003 - val_gender_output_loss: 0.4691 - val_image_quality_output_loss: 0.9750 - val_age_output_loss: 1.4091 - val_weight_output_loss: 0.9594 - val_bag_output_loss: 0.8729 - val_footwear_output_loss: 0.8756 - val_pose_output_loss: 0.6896 - val_emotion_output_loss: 0.9481 - val_gender_output_acc: 0.7722 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3780 - val_weight_output_acc: 0.6361 - val_bag_output_acc: 0.6023 - val_footwear_output_acc: 0.5927 - val_pose_output_acc: 0.7026 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 7.14806\n",
            "Epoch 14/20\n",
            "360/360 [==============================] - 228s 633ms/step - loss: 6.7581 - gender_output_loss: 0.4270 - image_quality_output_loss: 0.9542 - age_output_loss: 1.3618 - weight_output_loss: 0.9426 - bag_output_loss: 0.8254 - footwear_output_loss: 0.8051 - pose_output_loss: 0.5824 - emotion_output_loss: 0.8581 - gender_output_acc: 0.7969 - image_quality_output_acc: 0.5615 - age_output_acc: 0.4101 - weight_output_acc: 0.6368 - bag_output_acc: 0.6326 - footwear_output_acc: 0.6407 - pose_output_acc: 0.7552 - emotion_output_acc: 0.7159 - val_loss: 7.0842 - val_gender_output_loss: 0.4532 - val_image_quality_output_loss: 0.9714 - val_age_output_loss: 1.3968 - val_weight_output_loss: 0.9568 - val_bag_output_loss: 0.8699 - val_footwear_output_loss: 0.8396 - val_pose_output_loss: 0.6526 - val_emotion_output_loss: 0.9423 - val_gender_output_acc: 0.7782 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3821 - val_weight_output_acc: 0.6391 - val_bag_output_acc: 0.6084 - val_footwear_output_acc: 0.6074 - val_pose_output_acc: 0.7248 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00014: val_loss improved from 7.14806 to 7.08418, saving model to /content/saved_models/model_014.hdf5\n",
            "Epoch 15/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.7136 - gender_output_loss: 0.4177 - image_quality_output_loss: 0.9521 - age_output_loss: 1.3556 - weight_output_loss: 0.9411 - bag_output_loss: 0.8192 - footwear_output_loss: 0.7993 - pose_output_loss: 0.5697 - emotion_output_loss: 0.8571 - gender_output_acc: 0.8046 - image_quality_output_acc: 0.5533 - age_output_acc: 0.4106 - weight_output_acc: 0.6362 - bag_output_acc: 0.6355 - footwear_output_acc: 0.6446 - pose_output_acc: 0.7633 - emotion_output_acc: 0.7163\n",
            "360/360 [==============================] - 227s 631ms/step - loss: 6.7134 - gender_output_loss: 0.4176 - image_quality_output_loss: 0.9521 - age_output_loss: 1.3559 - weight_output_loss: 0.9406 - bag_output_loss: 0.8186 - footwear_output_loss: 0.7998 - pose_output_loss: 0.5694 - emotion_output_loss: 0.8578 - gender_output_acc: 0.8047 - image_quality_output_acc: 0.5534 - age_output_acc: 0.4107 - weight_output_acc: 0.6363 - bag_output_acc: 0.6359 - footwear_output_acc: 0.6445 - pose_output_acc: 0.7634 - emotion_output_acc: 0.7161 - val_loss: 7.1038 - val_gender_output_loss: 0.4345 - val_image_quality_output_loss: 0.9664 - val_age_output_loss: 1.4002 - val_weight_output_loss: 0.9638 - val_bag_output_loss: 0.8890 - val_footwear_output_loss: 0.8387 - val_pose_output_loss: 0.6574 - val_emotion_output_loss: 0.9521 - val_gender_output_acc: 0.7949 - val_image_quality_output_acc: 0.5590 - val_age_output_acc: 0.3715 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5958 - val_footwear_output_acc: 0.6129 - val_pose_output_acc: 0.7188 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 7.08418\n",
            "Epoch 16/20\n",
            "360/360 [==============================] - 228s 634ms/step - loss: 6.6733 - gender_output_loss: 0.4150 - image_quality_output_loss: 0.9447 - age_output_loss: 1.3537 - weight_output_loss: 0.9376 - bag_output_loss: 0.8169 - footwear_output_loss: 0.7945 - pose_output_loss: 0.5558 - emotion_output_loss: 0.8534 - gender_output_acc: 0.8071 - image_quality_output_acc: 0.5569 - age_output_acc: 0.4143 - weight_output_acc: 0.6382 - bag_output_acc: 0.6397 - footwear_output_acc: 0.6466 - pose_output_acc: 0.7700 - emotion_output_acc: 0.7160 - val_loss: 7.1782 - val_gender_output_loss: 0.4695 - val_image_quality_output_loss: 0.9829 - val_age_output_loss: 1.4138 - val_weight_output_loss: 0.9567 - val_bag_output_loss: 0.9710 - val_footwear_output_loss: 0.8268 - val_pose_output_loss: 0.6285 - val_emotion_output_loss: 0.9274 - val_gender_output_acc: 0.7802 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.3775 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5655 - val_footwear_output_acc: 0.6290 - val_pose_output_acc: 0.7419 - val_emotion_output_acc: 0.6835\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 7.08418\n",
            "Epoch 17/20\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.6135 - gender_output_loss: 0.4011 - image_quality_output_loss: 0.9385 - age_output_loss: 1.3497 - weight_output_loss: 0.9348 - bag_output_loss: 0.8117 - footwear_output_loss: 0.7820 - pose_output_loss: 0.5453 - emotion_output_loss: 0.8487 - gender_output_acc: 0.8140 - image_quality_output_acc: 0.5621 - age_output_acc: 0.4157 - weight_output_acc: 0.6387 - bag_output_acc: 0.6455 - footwear_output_acc: 0.6536 - pose_output_acc: 0.7756 - emotion_output_acc: 0.7162\n",
            "Epoch 00016: val_loss did not improve from 7.08418\n",
            "360/360 [==============================] - 228s 633ms/step - loss: 6.6154 - gender_output_loss: 0.4017 - image_quality_output_loss: 0.9382 - age_output_loss: 1.3502 - weight_output_loss: 0.9344 - bag_output_loss: 0.8118 - footwear_output_loss: 0.7824 - pose_output_loss: 0.5456 - emotion_output_loss: 0.8493 - gender_output_acc: 0.8136 - image_quality_output_acc: 0.5622 - age_output_acc: 0.4155 - weight_output_acc: 0.6388 - bag_output_acc: 0.6454 - footwear_output_acc: 0.6533 - pose_output_acc: 0.7755 - emotion_output_acc: 0.7159 - val_loss: 7.0655 - val_gender_output_loss: 0.4511 - val_image_quality_output_loss: 0.9622 - val_age_output_loss: 1.4082 - val_weight_output_loss: 0.9576 - val_bag_output_loss: 0.8843 - val_footwear_output_loss: 0.8487 - val_pose_output_loss: 0.6265 - val_emotion_output_loss: 0.9251 - val_gender_output_acc: 0.7858 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5922 - val_footwear_output_acc: 0.6099 - val_pose_output_acc: 0.7419 - val_emotion_output_acc: 0.6840\n",
            "\n",
            "Epoch 00017: val_loss improved from 7.08418 to 7.06551, saving model to /content/saved_models/model_017.hdf5\n",
            "Epoch 18/20\n",
            "360/360 [==============================]\n",
            "360/360 [==============================] - 227s 631ms/step - loss: 6.5728 - gender_output_loss: 0.3917 - image_quality_output_loss: 0.9330 - age_output_loss: 1.3474 - weight_output_loss: 0.9326 - bag_output_loss: 0.8068 - footwear_output_loss: 0.7841 - pose_output_loss: 0.5294 - emotion_output_loss: 0.8461 - gender_output_acc: 0.8215 - image_quality_output_acc: 0.5604 - age_output_acc: 0.4165 - weight_output_acc: 0.6362 - bag_output_acc: 0.6416 - footwear_output_acc: 0.6537 - pose_output_acc: 0.7813 - emotion_output_acc: 0.7161 - val_loss: 7.2503 - val_gender_output_loss: 0.5574 - val_image_quality_output_loss: 0.9579 - val_age_output_loss: 1.4222 - val_weight_output_loss: 0.9558 - val_bag_output_loss: 0.9251 - val_footwear_output_loss: 0.8620 - val_pose_output_loss: 0.6344 - val_emotion_output_loss: 0.9335 - val_gender_output_acc: 0.7263 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3553 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5771 - val_footwear_output_acc: 0.6139 - val_pose_output_acc: 0.7298 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 7.06551\n",
            "Epoch 19/20\n",
            "360/360 [==============================] - 228s 635ms/step - loss: 6.5436 - gender_output_loss: 0.3916 - image_quality_output_loss: 0.9288 - age_output_loss: 1.3449 - weight_output_loss: 0.9300 - bag_output_loss: 0.8009 - footwear_output_loss: 0.7760 - pose_output_loss: 0.5240 - emotion_output_loss: 0.8457 - gender_output_acc: 0.8240 - image_quality_output_acc: 0.5633 - age_output_acc: 0.4185 - weight_output_acc: 0.6382 - bag_output_acc: 0.6483 - footwear_output_acc: 0.6541 - pose_output_acc: 0.7896 - emotion_output_acc: 0.7161 - val_loss: 6.9623 - val_gender_output_loss: 0.4151 - val_image_quality_output_loss: 0.9584 - val_age_output_loss: 1.4037 - val_weight_output_loss: 0.9528 - val_bag_output_loss: 0.8764 - val_footwear_output_loss: 0.8312 - val_pose_output_loss: 0.5988 - val_emotion_output_loss: 0.9241 - val_gender_output_acc: 0.8054 - val_image_quality_output_acc: 0.5504 - val_age_output_acc: 0.3826 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5963 - val_footwear_output_acc: 0.6210 - val_pose_output_acc: 0.7480 - val_emotion_output_acc: 0.6850\n",
            "\n",
            "Epoch 00019: val_loss improved from 7.06551 to 6.96230, saving model to /content/saved_models/model_019.hdf5\n",
            "Epoch 20/20\n",
            "360/360 [==============================] - 227s 632ms/step - loss: 6.4941 - gender_output_loss: 0.3763 - image_quality_output_loss: 0.9229 - age_output_loss: 1.3420 - weight_output_loss: 0.9278 - bag_output_loss: 0.7971 - footwear_output_loss: 0.7693 - pose_output_loss: 0.5114 - emotion_output_loss: 0.8452 - gender_output_acc: 0.8323 - image_quality_output_acc: 0.5610 - age_output_acc: 0.4181 - weight_output_acc: 0.6370 - bag_output_acc: 0.6534 - footwear_output_acc: 0.6605 - pose_output_acc: 0.7940 - emotion_output_acc: 0.7170 - val_loss: 6.9288 - val_gender_output_loss: 0.4232 - val_image_quality_output_loss: 0.9625 - val_age_output_loss: 1.4050 - val_weight_output_loss: 0.9514 - val_bag_output_loss: 0.8537 - val_footwear_output_loss: 0.8273 - val_pose_output_loss: 0.5851 - val_emotion_output_loss: 0.9187 - val_gender_output_acc: 0.8009 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3805 - val_weight_output_acc: 0.6472 - val_bag_output_acc: 0.6220 - val_footwear_output_acc: 0.6290 - val_pose_output_acc: 0.7560 - val_emotion_output_acc: 0.6845\n",
            "\n",
            "Epoch 00020: val_loss improved from 6.96230 to 6.92878, saving model to /content/saved_models/model_020.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fiuk_ifL5c6y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "08ccf990-d96c-4ca8-b49b-f642801310ad"
      },
      "source": [
        "#plot_model_history(model_info)\n",
        "scores = model.evaluate_generator(generator=valid_gen, verbose =1)\n",
        "dict(zip(model.metrics_names, scores))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 9s 291ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.3805443548387097,\n",
              " 'age_output_loss': 1.4050314080330633,\n",
              " 'bag_output_acc': 0.6219758064516129,\n",
              " 'bag_output_loss': 0.8536572898587873,\n",
              " 'emotion_output_acc': 0.6844758064516129,\n",
              " 'emotion_output_loss': 0.9187358925419469,\n",
              " 'footwear_output_acc': 0.6290322580645161,\n",
              " 'footwear_output_loss': 0.8272802675923994,\n",
              " 'gender_output_acc': 0.8009072580645161,\n",
              " 'gender_output_loss': 0.42315275149960674,\n",
              " 'image_quality_output_acc': 0.5549395161290323,\n",
              " 'image_quality_output_loss': 0.9625127450112374,\n",
              " 'loss': 6.928777833138743,\n",
              " 'pose_output_acc': 0.7560483870967742,\n",
              " 'pose_output_loss': 0.5851286736226851,\n",
              " 'weight_output_acc': 0.6471774193548387,\n",
              " 'weight_output_loss': 0.9513695586112237}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YJOVSaT5csi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "#train_gen = PersonDataGenerator(train_df, batch_size=32, aug_list=[ImageDataGenerator(horizontal_flip=True)],\n",
        "#        incl_orig=False)  # Whether to include original images)\n",
        "#valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTXNbVOoovYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.load_weights(\"/content/saved_models/model_020.hdf5\")\n",
        "#opt = SGD(lr=0.001, momentum=0.95, nesterov=True)\n",
        "#model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]             )\n",
        "#model_info= model.fit_generator(generator=train_gen, validation_data=valid_gen,\n",
        "#    use_multiprocessing=True, workers=6, \n",
        "#    epochs=20, \n",
        "#    callbacks=callbacks, \n",
        "#    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}